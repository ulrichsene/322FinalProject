{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some useful mysklearn package import statements and reloads\n",
    "import importlib\n",
    "\n",
    "import mysklearn.utils\n",
    "importlib.reload(mysklearn.utils)\n",
    "from mysklearn import utils\n",
    "\n",
    "import mysklearn.mypytable\n",
    "importlib.reload(mysklearn.mypytable)\n",
    "from mysklearn.mypytable import MyPyTable \n",
    "\n",
    "import mysklearn.myclassifiers\n",
    "importlib.reload(mysklearn.myclassifiers)\n",
    "from mysklearn.myclassifiers import MyKNeighborsClassifier, MyDummyClassifier, MyNaiveBayesClassifier\n",
    "\n",
    "import mysklearn.myevaluation\n",
    "importlib.reload(mysklearn.myevaluation)\n",
    "import mysklearn.myevaluation as myevaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mysklearn.mypytable.MyPyTable at 0xffff7bf04f50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_diabetes_dataset = MyPyTable()\n",
    "pre_diabetes_dataset.load_from_file(\"output_data/cleaned_diabetes_data.csv\")\n",
    "# X, y = utils.prepare_mixed_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_diabetes_dataset.random_subsample_classes(\"output_data/diabetes_minimize.csv\", \"diabetes\", 1000, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Subsample Dataset (minimized for efficiency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mysklearn.mypytable.MyPyTable at 0xffff58786fc0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes_dataset = MyPyTable()\n",
    "diabetes_dataset.load_from_file(\"output_data/diabetes_minimize.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'str' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m knn \u001b[38;5;241m=\u001b[39m MyKNeighborsClassifier(\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     20\u001b[0m knn\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m---> 21\u001b[0m y_pred_knn \u001b[38;5;241m=\u001b[39m knn\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m     22\u001b[0m knn_accuracy \u001b[38;5;241m=\u001b[39m myevaluation\u001b[38;5;241m.\u001b[39maccuracy_score(y_test, y_pred_knn)\n\u001b[1;32m     23\u001b[0m knn_accuracies\u001b[38;5;241m.\u001b[39mappend(knn_accuracy)\n",
      "File \u001b[0;32m/home/finalProject/mysklearn/myclassifiers.py:253\u001b[0m, in \u001b[0;36mMyKNeighborsClassifier.predict\u001b[0;34m(self, x_test)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Makes predictions for test instances in X_test.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \n\u001b[1;32m    245\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;124;03m    y_predicted(list of obj): The predicted target y values (parallel to X_test)\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# need to call the kneighbors method to get the index of the k nearest neighbors\u001b[39;00m\n\u001b[0;32m--> 253\u001b[0m neighbor_indexes, distances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkneighbors(x_test)\n\u001b[1;32m    254\u001b[0m y_predicted \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    256\u001b[0m \u001b[38;5;66;03m# next need to find the corresponding y_train values for the nearest neighbors (classification)\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# perform majority voting to determine most common class label among neighbors\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# return the predicted class label\u001b[39;00m\n",
      "File \u001b[0;32m/home/finalProject/mysklearn/myclassifiers.py:226\u001b[0m, in \u001b[0;36mMyKNeighborsClassifier.kneighbors\u001b[0;34m(self, x_test, n_neighbors)\u001b[0m\n\u001b[1;32m    223\u001b[0m row_index_distance \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;66;03m# list to store distances for current test instance\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, train_instance \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_train): \u001b[38;5;66;03m# calculate distance between test instance and all training instances\u001b[39;00m\n\u001b[0;32m--> 226\u001b[0m     distance \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mcompute_mixed_euclidean_distance(train_instance, test_instance, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategorical)\n\u001b[1;32m    227\u001b[0m     row_index_distance\u001b[38;5;241m.\u001b[39mappend((i, distance))\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m# sort distances and get the k closest neighbors\u001b[39;00m\n",
      "File \u001b[0;32m/home/finalProject/mysklearn/utils.py:304\u001b[0m, in \u001b[0;36mcompute_mixed_euclidean_distance\u001b[0;34m(v1, v2, categorical)\u001b[0m\n\u001b[1;32m    302\u001b[0m             distance \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# if the attribute is numerical, add the squared difference\u001b[39;00m\n\u001b[0;32m--> 304\u001b[0m         difference \u001b[38;5;241m=\u001b[39m v1[i] \u001b[38;5;241m-\u001b[39m v2[i]\n\u001b[1;32m    305\u001b[0m         distance \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m difference \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m distance \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'str'"
     ]
    }
   ],
   "source": [
    "X = [[row[0], row[1], row[2], row[3], row[4], row[5], row[6], row[7]] for row in diabetes_dataset.data]\n",
    "y = [row[8] for row in diabetes_dataset.data]\n",
    "# print(y)\n",
    "# blood glucose level, hb1ac level, smoking history, gender, bmi, age, heart disease\n",
    "categorical = [False, False, False, True, True, True, True]\n",
    "folds = myevaluation.stratified_kfold_split(X, y, n_splits=10, random_state=1)\n",
    "# knn_classifier = MyKNeighborsClassifier(n_neighbors=3)\n",
    "# knn_classifier.fit(X, y)\n",
    "# knn_classifier.kneighbors()\n",
    "# print(folds)\n",
    "knn_accuracies = []\n",
    "naive_accuracies = []\n",
    "dummy_accuracies = []\n",
    "\n",
    "for train_indices, test_indices in folds:\n",
    "        X_train, X_test = [X[i] for i in train_indices], [X[i] for i in test_indices]\n",
    "        y_train, y_test = [y[i] for i in train_indices], [y[i] for i in test_indices]\n",
    "\n",
    "        knn = MyKNeighborsClassifier(10)\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred_knn = knn.predict(X_test)\n",
    "        knn_accuracy = myevaluation.accuracy_score(y_test, y_pred_knn)\n",
    "        knn_accuracies.append(knn_accuracy)\n",
    "\n",
    "        naive = MyNaiveBayesClassifier()\n",
    "        naive.fit(X_train, y_train)\n",
    "        y_pred_naive = naive.predict(X_test)\n",
    "        naive_accuracy = myevaluation.accuracy_score(y_test, y_pred_naive)\n",
    "        naive_accuracies.append(naive_accuracy)\n",
    "\n",
    "        dummy = MyDummyClassifier()\n",
    "        dummy.fit(X_train, y_train)\n",
    "        y_pred_dummy = dummy.predict(X_test)\n",
    "        dummy_accuracy = myevaluation.accuracy_score(y_test, y_pred_dummy)\n",
    "        dummy_accuracies.append(dummy_accuracy)\n",
    "\n",
    "print(knn_accuracies)\n",
    "\n",
    "print(\"========================================================\")\n",
    "print(\"10-Fold Cross Validation Results for KNN Classifier\")\n",
    "print(\"========================================================\")\n",
    "results = utils.evaluate_classifier(X, y, knn)\n",
    "knn_average_accuracy = sum(knn_accuracies) / 10\n",
    "knn_error_rate = 1 - knn_average_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "Predictive Accuracy\n",
      "===========================================\n",
      "10-Fold Cross Validation\n",
      "k Nearest Neighbors Classifier: accuracy = 0.81, error rate = 0.19\n"
     ]
    }
   ],
   "source": [
    "print(\"===========================================\")\n",
    "print(\"Predictive Accuracy\")\n",
    "print(\"===========================================\")\n",
    "print(\"10-Fold Cross Validation\")\n",
    "print(\"k Nearest Neighbors Classifier: accuracy = {:.2f}, error rate = {:.2f}\".format(knn_average_accuracy, knn_error_rate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "10-Fold Cross Validation Results for Dummy Classifier\n",
      "========================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m10-Fold Cross Validation Results for Dummy Classifier\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m========================================================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m results \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mevaluate_classifier(X, y, dummy_classifier)\n",
      "File \u001b[0;32m/home/finalProject/mysklearn/utils.py:689\u001b[0m, in \u001b[0;36mevaluate_classifier\u001b[0;34m(X, y, classifier, pos_label, n_splits)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_classifier\u001b[39m(X, y, classifier, pos_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m, n_splits \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m    676\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" This function will return the evaluation scores for various metrics for \u001b[39;00m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;124;03m        each classifier.\u001b[39;00m\n\u001b[1;32m    678\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    687\u001b[0m \n\u001b[1;32m    688\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 689\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m cross_val_predict(X, y, classifier, n_splits\u001b[38;5;241m=\u001b[39mn_splits)\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;66;03m# calculate metrics\u001b[39;00m\n\u001b[1;32m    692\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m myevaluation\u001b[38;5;241m.\u001b[39maccuracy_score(y, predictions)\n",
      "File \u001b[0;32m/home/finalProject/mysklearn/utils.py:524\u001b[0m, in \u001b[0;36mcross_val_predict\u001b[0;34m(X, y, classifier, n_splits, random_state, shuffle)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m train_indexes:\n\u001b[1;32m    523\u001b[0m     X_train\u001b[38;5;241m.\u001b[39mappend(X[i])\n\u001b[0;32m--> 524\u001b[0m     y_train\u001b[38;5;241m.\u001b[39mappend(y[i])\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m test_indexes:\n\u001b[1;32m    526\u001b[0m     X_test\u001b[38;5;241m.\u001b[39mappend(X[i])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# X, y = utils.prepare_mixed_data()\n",
    "# dummy_classifier = MyDummyClassifier()\n",
    "\n",
    "print(\"========================================================\")\n",
    "print(\"10-Fold Cross Validation Results for Dummy Classifier\")\n",
    "print(\"========================================================\")\n",
    "results = utils.evaluate_classifier(X, y, dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "10-Fold Cross Validation Results for Naive Bayes Classifier\n",
      "========================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m10-Fold Cross Validation Results for Naive Bayes Classifier\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m========================================================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m results \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mevaluate_classifier(X, y, naive_classifier)\n",
      "File \u001b[0;32m/home/322FinalProject/mysklearn/utils.py:686\u001b[0m, in \u001b[0;36mevaluate_classifier\u001b[0;34m(X, y, classifier, pos_label, n_splits)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_classifier\u001b[39m(X, y, classifier, pos_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m, n_splits \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m    673\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" This function will return the evaluation scores for various metrics for \u001b[39;00m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;124;03m        each classifier.\u001b[39;00m\n\u001b[1;32m    675\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    684\u001b[0m \n\u001b[1;32m    685\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 686\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m cross_val_predict(X, y, classifier, n_splits\u001b[38;5;241m=\u001b[39mn_splits)\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;66;03m# calculate metrics\u001b[39;00m\n\u001b[1;32m    689\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m myevaluation\u001b[38;5;241m.\u001b[39maccuracy_score(y, predictions)\n",
      "File \u001b[0;32m/home/322FinalProject/mysklearn/utils.py:521\u001b[0m, in \u001b[0;36mcross_val_predict\u001b[0;34m(X, y, classifier, n_splits, random_state, shuffle)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m train_indexes:\n\u001b[1;32m    520\u001b[0m     X_train\u001b[38;5;241m.\u001b[39mappend(X[i])\n\u001b[0;32m--> 521\u001b[0m     y_train\u001b[38;5;241m.\u001b[39mappend(y[i])\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m test_indexes:\n\u001b[1;32m    523\u001b[0m     X_test\u001b[38;5;241m.\u001b[39mappend(X[i])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# X, y = utils.prepare_mixed_data()\n",
    "# naive_classifier = MyNaiveBayesClassifier()\n",
    "\n",
    "print(\"========================================================\")\n",
    "print(\"10-Fold Cross Validation Results for Naive Bayes Classifier\")\n",
    "print(\"========================================================\")\n",
    "results = utils.evaluate_classifier(X, y, naive)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
