{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some useful mysklearn package import statements and reloads\n",
    "import importlib\n",
    "\n",
    "import mysklearn.utils\n",
    "importlib.reload(mysklearn.utils)\n",
    "from mysklearn import utils\n",
    "\n",
    "import mysklearn.mypytable\n",
    "importlib.reload(mysklearn.mypytable)\n",
    "from mysklearn.mypytable import MyPyTable \n",
    "\n",
    "import mysklearn.myclassifiers\n",
    "importlib.reload(mysklearn.myclassifiers)\n",
    "from mysklearn.myclassifiers import MyKNeighborsClassifier, MyDummyClassifier, MyNaiveBayesClassifier\n",
    "\n",
    "import mysklearn.myevaluation\n",
    "importlib.reload(mysklearn.myevaluation)\n",
    "import mysklearn.myevaluation as myevaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mysklearn.mypytable.MyPyTable at 0xffff4e76d0d0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_diabetes_dataset = MyPyTable()\n",
    "pre_diabetes_dataset.load_from_file(\"output_data/cleaned_diabetes_data.csv\")\n",
    "# X, y = utils.prepare_mixed_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_diabetes_dataset.random_subsample_classes(\"output_data/diabetes_minimize.csv\", \"diabetes\", 1000, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Subsample Dataset (minimized for efficiency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mysklearn.mypytable.MyPyTable at 0xffff4c16f320>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes_dataset = MyPyTable()\n",
    "diabetes_dataset.load_from_file(\"output_data/diabetes_minimize.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "10-Fold Cross Validation Results for KNN Classifier\n",
      "========================================================\n"
     ]
    }
   ],
   "source": [
    "X = [[row[0], row[1], row[2], row[3], row[4], row[5], row[6], row[7]] for row in diabetes_dataset.data]\n",
    "y = [row[8] for row in diabetes_dataset.data]\n",
    "# print(y)\n",
    "# blood glucose level, hb1ac level, smoking history, gender, bmi, age, heart disease\n",
    "categorical = [False, False, False, True, True, True, True]\n",
    "folds = myevaluation.stratified_kfold_split(X, y, n_splits=10, random_state=1)\n",
    "# knn_classifier = MyKNeighborsClassifier(n_neighbors=3)\n",
    "# knn_classifier.fit(X, y)\n",
    "# knn_classifier.kneighbors()\n",
    "# print(folds)\n",
    "knn_accuracies = []\n",
    "\n",
    "for train_indices, test_indices in folds:\n",
    "        X_train, X_test = [X[i] for i in train_indices], [X[i] for i in test_indices]\n",
    "        y_train, y_test = [y[i] for i in train_indices], [y[i] for i in test_indices]\n",
    "\n",
    "        knn = MyKNeighborsClassifier(10)\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred_knn = knn.predict(X_test)\n",
    "        knn_accuracy = myevaluation.accuracy_score(y_test, y_pred_knn)\n",
    "        knn_accuracies.append(knn_accuracy)\n",
    "\n",
    "# print(knn_accuracies)\n",
    "\n",
    "print(\"========================================================\")\n",
    "print(\"10-Fold Cross Validation Results for KNN Classifier\")\n",
    "print(\"========================================================\")\n",
    "# results = utils.evaluate_classifier(X, y, knn)\n",
    "knn_average_accuracy = sum(knn_accuracies) / 10\n",
    "knn_error_rate = 1 - knn_average_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "Predictive Accuracy\n",
      "===========================================\n",
      "10-Fold Cross Validation\n",
      "k Nearest Neighbors Classifier: accuracy = 0.81, error rate = 0.19\n"
     ]
    }
   ],
   "source": [
    "print(\"===========================================\")\n",
    "print(\"Predictive Accuracy\")\n",
    "print(\"===========================================\")\n",
    "print(\"10-Fold Cross Validation\")\n",
    "print(\"k Nearest Neighbors Classifier: accuracy = {:.2f}, error rate = {:.2f}\".format(knn_average_accuracy, knn_error_rate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "10-Fold Cross Validation Results for Dummy Classifier\n",
      "========================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m10-Fold Cross Validation Results for Dummy Classifier\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m========================================================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m results \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mevaluate_classifier(X, y, dummy_classifier)\n",
      "File \u001b[0;32m/home/finalProject/mysklearn/utils.py:689\u001b[0m, in \u001b[0;36mevaluate_classifier\u001b[0;34m(X, y, classifier, pos_label, n_splits)\u001b[0m\n\u001b[1;32m    687\u001b[0m if row_total > 0:\n\u001b[1;32m    688\u001b[0m     recognition = (row[i] / row_total) * 100\n\u001b[0;32m--> 689\u001b[0m else:\n\u001b[1;32m    690\u001b[0m     recognition = 0\n\u001b[1;32m    692\u001b[0m # Add the \"Actual Class\" column at the beginning of each row\n",
      "File \u001b[0;32m/home/finalProject/mysklearn/utils.py:527\u001b[0m, in \u001b[0;36mcross_val_predict\u001b[0;34m(X, y, classifier, n_splits, random_state, shuffle)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcross_val_predict\u001b[39m(X, y, classifier, n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    525\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generates cross-validated predictions from the input classifier.\u001b[39;00m\n\u001b[1;32m    526\u001b[0m \n\u001b[0;32m--> 527\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;124;03m        X (list of list of obj): Features of the dataset.\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;124;03m        y (list of obj): Target labels of the dataset.\u001b[39;00m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;124;03m        classifier: An instance of the classifier with fit and predict methods.\u001b[39;00m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;124;03m        n_splits (int): Number of folds for cross-validation.\u001b[39;00m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;124;03m        random_state (int): Seed for random number generator for reproducibility.\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;124;03m        shuffle (bool): Whether to shuffle the data before splitting.\u001b[39;00m\n\u001b[1;32m    534\u001b[0m \n\u001b[1;32m    535\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;124;03m        predictions (list of obj): Predicted labels for each instance in the dataset.\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    539\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(y)\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;66;03m# call kfold_split to get number of splits\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X, y = utils.prepare_mixed_data()\n",
    "dummy_classifier = MyDummyClassifier()\n",
    "\n",
    "print(\"========================================================\")\n",
    "print(\"10-Fold Cross Validation Results for Dummy Classifier\")\n",
    "print(\"========================================================\")\n",
    "results = utils.evaluate_classifier(X, y, dummy_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "10-Fold Cross Validation Results for Naive Bayes Classifier\n",
      "========================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m10-Fold Cross Validation Results for Naive Bayes Classifier\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m========================================================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m results \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mevaluate_classifier(X, y, naive_classifier)\n",
      "File \u001b[0;32m/home/322FinalProject/mysklearn/utils.py:686\u001b[0m, in \u001b[0;36mevaluate_classifier\u001b[0;34m(X, y, classifier, pos_label, n_splits)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_classifier\u001b[39m(X, y, classifier, pos_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m, n_splits \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m    673\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" This function will return the evaluation scores for various metrics for \u001b[39;00m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;124;03m        each classifier.\u001b[39;00m\n\u001b[1;32m    675\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    684\u001b[0m \n\u001b[1;32m    685\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 686\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m cross_val_predict(X, y, classifier, n_splits\u001b[38;5;241m=\u001b[39mn_splits)\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;66;03m# calculate metrics\u001b[39;00m\n\u001b[1;32m    689\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m myevaluation\u001b[38;5;241m.\u001b[39maccuracy_score(y, predictions)\n",
      "File \u001b[0;32m/home/322FinalProject/mysklearn/utils.py:521\u001b[0m, in \u001b[0;36mcross_val_predict\u001b[0;34m(X, y, classifier, n_splits, random_state, shuffle)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m train_indexes:\n\u001b[1;32m    520\u001b[0m     X_train\u001b[38;5;241m.\u001b[39mappend(X[i])\n\u001b[0;32m--> 521\u001b[0m     y_train\u001b[38;5;241m.\u001b[39mappend(y[i])\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m test_indexes:\n\u001b[1;32m    523\u001b[0m     X_test\u001b[38;5;241m.\u001b[39mappend(X[i])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X, y = utils.prepare_mixed_data()\n",
    "naive_classifier = MyNaiveBayesClassifier()\n",
    "\n",
    "print(\"========================================================\")\n",
    "print(\"10-Fold Cross Validation Results for Naive Bayes Classifier\")\n",
    "print(\"========================================================\")\n",
    "results = utils.evaluate_classifier(X, y, naive_classifier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
